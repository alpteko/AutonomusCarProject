{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from cv2 import imread,cvtColor\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('driver_imgs_list.csv')\n",
    "lst = csv.reader(file)\n",
    "file_list = []\n",
    "for i in lst:\n",
    "    file_list.append(i)\n",
    "file_list.pop(0)\n",
    "w = 96\n",
    "h = 128\n",
    "input_size = w * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    img = cv2.imread('train/'+file[1]+'/'+file[2],0)\n",
    "    img = cv2.resize(img,(w,h), interpolation=cv2.INTER_LINEAR)\n",
    "    img = img.astype(np.float32)/255.0\n",
    "    label = int(file[1][1])\n",
    "    return img.reshape([1,-1]),label\n",
    "def show(array):\n",
    "    plt.figure()\n",
    "    plt.imshow(array.reshape([h,w]), cmap=\"gray\")\n",
    "    plt.show()\n",
    "def get_batch(batch_size):\n",
    "    j = np.random.randint(0,20788,batch_size)\n",
    "    batch = np.zeros((batch_size,w*h))\n",
    "    k = 0\n",
    "    label = np.zeros((batch_size,10))\n",
    "    for i in j:\n",
    "        batch[k:k+1,:],lab = read_file(file_list[i])\n",
    "        label[k,lab] = 1\n",
    "        k+=1\n",
    "    return batch,label\n",
    "def get_test_batch(batch_size):\n",
    "    j = np.random.randint(20788,22424,batch_size)\n",
    "    batch = np.zeros((batch_size,w*h))\n",
    "    k = 0\n",
    "    label = np.zeros((batch_size,10))\n",
    "    for i in j:\n",
    "        batch[k:k+1,:],lab = read_file(file_list[i])\n",
    "        label[k,lab] = 1\n",
    "        k+=1\n",
    "    return batch,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## num_steps = 500000\n",
    "batch_size = 64\n",
    "\n",
    "# Network Parameters\n",
    "image_dim = input_size # MNIST images are 28x28 pixels\n",
    "hidden_dim = 4000\n",
    "latent_dim = 2000\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(glorot_init([image_dim, hidden_dim])),\n",
    "    'z_mean': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
    "    'z_std': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
    "    'decoder_h1': tf.Variable(glorot_init([latent_dim, hidden_dim])),\n",
    "    'decoder_out': tf.Variable(glorot_init([hidden_dim, image_dim]))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
    "    'z_mean': tf.Variable(glorot_init([latent_dim])),\n",
    "    'z_std': tf.Variable(glorot_init([latent_dim])),\n",
    "    'decoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
    "    'decoder_out': tf.Variable(glorot_init([image_dim]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "input_image = tf.placeholder(tf.float32, shape=[None, image_dim])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,10])\n",
    "encoder = tf.layers.dense(inputs=input_image,units=hidden_dim,activation=tf.nn.tanh)\n",
    "z_mean = tf.layers.dense(inputs=encoder,units=latent_dim)\n",
    "z_std =  tf.layers.dense(inputs=encoder,units=latent_dim)\n",
    "\n",
    "# Sampler: Normal (gaussian) random distribution\n",
    "eps = tf.random_normal(tf.shape(z_std), dtype=tf.float32, mean=0., stddev=1.0,\n",
    "                       name='epsilon')\n",
    "z = z_mean + tf.exp(z_std / 2) * eps\n",
    "\n",
    "y_1 = tf.layers.dense(inputs=z_mean,units=500,name='alp',activation=tf.nn.sigmoid,kernel_initializer=tf.variance_scaling_initializer)\n",
    "y_11 = tf.layers.dense(inputs=y_1,units=100,name='alp2',activation=tf.nn.sigmoid,kernel_initializer=tf.variance_scaling_initializer)\n",
    "y_2 = tf.layers.dense(inputs=y_11,units=10,name='alp3')\n",
    "clf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=y_2))\n",
    "alper_list = [v for v in tf.trainable_variables() if 'alp' in v.name]\n",
    "lrr = tf.placeholder(shape=[],dtype=tf.float32)\n",
    "optimizer2 = tf.train.RMSPropOptimizer(lrr).minimize(clf_loss,var_list=alper_list)\n",
    "correctPred = tf.equal(tf.argmax(y_2,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "\n",
    "# Building the decoder (with scope to re-use these layers later)\n",
    "decoder_h1 = tf.layers.dense(inputs=z,units=hidden_dim,activation=tf.nn.tanh)\n",
    "decoder = tf.layers.dense(inputs=decoder_h1,units=input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE Loss\n",
    "def vae_loss(x_reconstructed, x_true):\n",
    "    # Reconstruction loss\n",
    "    #encode_decode_loss = x_true * tf.log(1e-5 + x_reconstructed) \\\n",
    "    #                     + (1 - x_true) * tf.log(1e-5 + 1 - x_reconstructed)\n",
    "    #encode_decode_loss = -tf.reduce_sum(encode_decode_loss, 1)\n",
    "    encode_decode_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=x_true,logits=x_reconstructed))\n",
    "    # KL Divergence loss\n",
    "    kl_div_loss = 1 + z_std - tf.square(z_mean) - tf.exp(z_std)\n",
    "    kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)\n",
    "    print(encode_decode_loss,kl_div_loss)\n",
    "    return tf.reduce_mean(encode_decode_loss  + kl_div_loss * 0.0005)\n",
    "\n",
    "print(decoder.shape)\n",
    "print(input_image.shape)\n",
    "loss_op = vae_loss(decoder, input_image)\n",
    "optimizer = tf.train.RMSPropOptimizer(lrr)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "loss_array = []\n",
    "clf_array = []\n",
    "learn =0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "while(1): \n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    if 1:\n",
    "        batch_x,_ = get_batch(128)\n",
    "        feed_dict = {input_image: batch_x,lrr:learn*0.001}\n",
    "        _, l = sess.run([train_op, loss_op], feed_dict=feed_dict)\n",
    "        loss_array.append(l)\n",
    "        print('Recons:',l)\n",
    "    if 0 :\n",
    "        batch_x,batch_y = get_batch(128)\n",
    "        feed_dict = {input_image: batch_x,Y:batch_y,lrr:learn}\n",
    "        _, l = sess.run([optimizer2, clf_loss], feed_dict=feed_dict)\n",
    "        clf_array.append(l)\n",
    "        print('Class:',l)\n",
    "    if i % 10 == 0:\n",
    "        #show(batch_x[0:1,:])\n",
    "        print('Step %i, Loss: %f' % (i, l))\n",
    "        show(batch_x[0:1,:])\n",
    "        img = sess.run(decoder,feed_dict=feed_dict)\n",
    "        show(img[0:1,:])\n",
    "        learn *= 1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "for i in range(100):\n",
    "        batch_x,label = get_test_batch(batch_size)\n",
    "        feed_dict = {input_image: batch_x,Y:label}\n",
    "        l,acc = sess.run([clf_loss,accuracy], feed_dict=feed_dict)\n",
    "        total_acc += acc\n",
    "print(total_acc/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(loss_array,label='Recons')\n",
    "#plt.semilogy(clf_array,label='Class')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
